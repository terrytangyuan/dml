% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dca.R
\name{dca}
\alias{dca}
\title{Discriminative Component Analysis}
\usage{
dca(data, chunks, neglinks, useD = NULL)
}
\arguments{
\item{data}{\code{n * d} data matrix. \code{n} is the number of data points,
\code{d} is the dimension of the data.
Each data point is a row in the matrix.}

\item{chunks}{length \code{n} vector describing the chunklets:
\code{-1} in the \code{i} th place means point \code{i}
doesn't belong to any chunklet;
integer \code{j} in place \code{i} means point \code{i}
belongs to chunklet j.
The chunklets indexes should be 1:(number of chunklets).}

\item{neglinks}{\code{s * s} symmetric matrix describing the negative relationship
between all the \code{s} chunklets.
For the element \eqn{neglinks_{ij}}:
\eqn{neglinks_{ij} = 1} means chunklet \code{i} and chunklet {j}
have negative constraint(s);
\eqn{neglinks_{ij} = 0} means chunklet \code{i} and chunklet {j}
don't have negative constraints
or we don't have information about that.}

\item{useD}{Integer. Optional. When not given, DCA is done in the
original dimension and B is full rank. When useD is given,
DCA is preceded by constraints based LDA which reduces the
dimension to useD. B in this case is of rank useD.}
}
\value{
list of the DCA results:
\item{B}{DCA suggested Mahalanobis matrix}
\item{DCA}{DCA suggested transformation of the data.
           The dimension is (original data dimension) * (useD)}
\item{newData}{DCA transformed data}

For every two original data points (x1, x2) in newData (y1, y2):

\eqn{(x2 - x1)' * B * (x2 - x1) = || (x2 - x1) * A ||^2 = || y2 - y1 ||^2}
}
\description{
Performs discriminative component analysis on the given data.
}
\details{
Put DCA function details here.
}
\note{
Put some note here.
}
\examples{
\dontrun{
# generate synthetic Gaussian data
library("MASS")

k <- 100 # sample size of each class
n <- 3 # specify how many class
N <- k * n # total sample number

set.seed(123)
x1 <- mvrnorm(k, mu = c(-10, 6), matrix(c(10, 4, 4, 10), ncol = 2))
x2 <- mvrnorm(k, mu = c(0, 0), matrix(c(10, 4, 4, 10), ncol = 2))
x3 <- mvrnorm(k, mu = c(10, -6), matrix(c(10, 4, 4, 10), ncol = 2))
data <- as.data.frame(rbind(x1, x2, x3))

# The fully labeled data set with 3 classes
plot(data$V1, data$V2,
     bg = c("#E41A1C", "#377EB8", "#4DAF4A")[gl(n, k)],
     pch = c(rep(22, k), rep(21, k), rep(25, k))
)
Sys.sleep(3)

# The same data unlabeled; clearly the class structure is less evident
plot(data$V1, data$V2)
Sys.sleep(3)

chunk1 <- sample(1:100, 5)
chunk2 <- sample(setdiff(1:100, chunk1), 5)
chunk3 <- sample(101:200, 5)
chunk4 <- sample(setdiff(101:200, chunk3), 5)
chunk5 <- sample(201:300, 5)
chks <- list(chunk1, chunk2, chunk3, chunk4, chunk5)
chunks <- rep(-1, 300)

# positive samples in the chunks
for (i in 1:5) {
  for (j in chks[[i]]) {
    chunks[j] <- i
  }
}

# define the negative constrains between chunks
neglinks <- matrix(c(
  0, 0, 1, 1, 1,
  0, 0, 1, 1, 1,
  1, 1, 0, 0, 0,
  1, 1, 0, 0, 1,
  1, 1, 1, 1, 0
),
ncol = 5, byrow = TRUE
)

dcaData <- dca(data = data, chunks = chunks, neglinks = neglinks)$newData

# plot DCA transformed data
plot(dcaData[, 1], dcaData[, 2],
     bg = c("#E41A1C", "#377EB8", "#4DAF4A")[gl(n, k)],
     pch = c(rep(22, k), rep(21, k), rep(25, k)),
     xlim = c(-15, 15), ylim = c(-15, 15)
)}
}
\references{
Steven C.H. Hoi, W. Liu, M.R. Lyu and W.Y. Ma (2006).
Learning Distance Metrics with Contextual Constraints for Image Retrieval.
\emph{Proceedings IEEE Conference on Computer Vision and Pattern Recognition
(CVPR2006)}.
}
\keyword{component}
\keyword{dca}
\keyword{discriminant}
\keyword{mahalanobis}
\keyword{metric}
\keyword{transformation}
